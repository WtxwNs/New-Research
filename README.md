# 2025年中期以来NLP与理论ML前沿进展概览

## 1. 新兴方向：世界模型与AI智能体的革命性突破

### 1.1 世界模型：从被动观察到主动理解物理世界

2025年下半年，人工智能领域，特别是世界模型（World Models）的研究，经历了一次深刻的范式转变。传统的视频生成模型，如OpenAI的Sora，虽然在视觉逼真度上取得了惊人成就，但其本质仍停留在“被动观察”阶段。这些模型通过学习海量网络视频数据，掌握了事物的外观和表面动态，却未能理解驱动这些现象的内在物理规律。这导致它们生成的视频常常出现违反物理常识的错误，例如物体悬浮或穿透其他物体 。这种局限性源于其学习方式——仅仅通过“看”视频，而非像人类一样通过与世界互动来建立因果和物理直觉。正如认知科学家让·皮亚杰所言：“要了解一个物体，就必须对它采取行动” 。这一洞察催生了新一代世界模型的研究，其核心目标是从被动的视觉模仿转向主动的、基于交互的物理理解，让AI不仅能“看到”世界，更能“读懂”并“预测”世界的运行法则。

#### 1.1.1 北京大学WoW模型：首个真正理解物理规律的AI系统

2025年10月，由北京大学计算机学院、北京人形机器人创新中心及香港科技大学联合发布的研究成果——**WoW（World-Omniscient World Model）世界模型**，标志着这一范式转变的重大突破 。WoW被设计为一个拥有**140亿参数**的生成式世界模型，其革命性在于其训练方式：它并非依赖于被动的视频观察，而是通过分析**200万个真实的机器人交互轨迹**进行学习 。这些数据涵盖了**5275个不同任务**和**12种不同类型的机器人**，相当于让AI通过“亲手做实验”来亲身体验物理世界，从而真正内化重力、碰撞、惯性等核心物理定律 。这种基于具身交互（Embodied Interaction）的学习方法，使得WoW不仅能生成视觉上逼真的视频，更重要的是，其生成的内容在物理上是合理且一致的，从根本上解决了传统视频生成模型的物理幻觉问题 。

##### 1.1.1.1 核心创新：SOPHIA自我优化框架

WoW模型的核心创新在于其独特的**SOPHIA框架（Self-Optimizing Predictive Hallucination Improving Agent）** ，这是一个让AI具备自我反思与纠错能力的闭环系统 。SOPHIA的运作机制模拟了人类的认知过程，即“想象-验证-修正-再想象”的循环。具体而言，该框架包含三个关键阶段：
1.  **任务想象（Task Imagination）** ：模型首先根据指令生成一个关于未来状态的初步预测或“想象”，这通常以像素级的未来视频帧形式呈现。
2.  **经验反思（Experience Reflection）** ：一个内置的视觉-语言模型（VLM）代理扮演“内在老师”的角色，对生成的预测进行物理一致性评估。它会检查视频是否违反了物理规律，例如物体是否凭空消失或运动轨迹是否违背重力。
3.  **行为提取与修正（Action Extraction & Refinement）** ：如果评估发现问题，VLM代理会生成具体的修正建议，例如调整语言指令或中间推理步骤，并引导模型重新生成一个更合理的预测。这个过程会反复迭代，直到生成的视频在物理上足够精确 。

这种“生成-评估-修正”的认知闭环，使得WoW能够主动地将涌现出的物理直觉约束到符合现实世界的轨道上，从而不断提升其预测的准确性和物理合理性 。这不仅是技术上的突破，更是在模拟人类高级认知功能方面迈出的重要一步。

##### 1.1.1.2 性能评估：WoWBench基准测试

为了系统性地评估WoW的物理理解能力，研究团队开发了**WoWBench**，这是首个专门用于评估AI物理理解能力的综合性基准测试 。该基准包含**606个**精心设计的测试样本，覆盖了多个评估维度：
*   **预测推理（Predictive Reasoning）** ：评估模型对核心物理原理的掌握，如物体恒存性（Object Permanence）、碰撞动力学（Collision Dynamics）和轨迹合理性（Trajectory Plausibility）。
*   **决策与规划（Decision & Planning）** ：通过要求模型为复杂的长时程任务生成连贯的视频序列，评估其隐式的任务分解和因果依赖理解能力。
*   **泛化执行（Generalization）** ：测试模型在分布外（Out-of-Distribution, OOD）数据上的表现，例如从未见过的艺术画作或经过风格迁移的图像，以验证其物理理解是否具有普适性。

在WoWBench上的实验结果显示，**WoW在指令理解方面达到了96.53%的准确率，在物理定律理解方面达到了80.16%的准确率**，在多项测试中均显著优于现有模型，确立了新的行业标杆 。

##### 1.1.1.3 从想象到行动：实现具身智能的闭环

WoW模型的另一项关键成就在于它成功地将“想象”与“行动”连接起来，实现了从世界模型到物理执行的完整闭环 。研究团队通过一个联合训练的逆动力学模型（Inverse Dynamics Model），特别是其创新的**Flow-Mask Inverse Dynamics Model (FM-IDM)** ，将WoW生成的优化后未来轨迹转化为机器人可以执行的低级动作指令 。实验表明，该系统在真实世界机器人任务中取得了卓越的成功率：**在简单任务上达到94.5%，在中等难度任务上达到75.2%** 。这证明了WoW的“想象”不仅仅是视觉上的幻觉，而是深深植根于物理现实的、可执行的计划。这一突破为开发能够真正在物理世界中自主行动和学习的具身智能体奠定了坚实的基础。

#### 1.1.2 OpenAI Sora 2：作为世界模拟系统的音视频生成模型

在2025年9月30日，OpenAI正式发布了其最新的视频和音频生成模型——**Sora 2**，这一发布被广泛认为是AI视频生成领域的“GPT-3.5时刻” 。与2024年2月发布的初代Sora（被视为“GPT-1时刻”）相比，Sora 2在物理准确性、逼真度、可控性以及音视频同步方面实现了质的飞跃 。OpenAI将Sora 2定位为一个先进的“世界模拟系统”（world simulation system），它不仅能生成视觉上连贯的视频，更重要的是，它能够理解和模拟物理世界的动态规律，包括物体的运动、声音的传播以及它们之间的相互作用 。这一突破标志着生成式AI从单纯的像素生成，迈向了对物理世界进行内在建模和预测的新阶段，为AI智能体在虚拟环境中进行学习和交互提供了前所未有的强大平台。

Sora 2的核心技术突破在于其增强的“世界建模”（world modeling）能力。通过在海量视频数据上进行大规模预训练和后期训练，模型学会了预测在特定物理情境下，下一帧画面应该如何呈现 。这使得Sora 2能够生成更符合物理定律的场景。例如，在演示中，当一名篮球运动员投篮未中时，球会真实地反弹 off the backboard，而不是像早期模型那样为了迎合文本提示而“作弊”，将球瞬间传送到篮筐中 。同样，模型能够逼真地模拟复杂的物理现象，如奥运级别的体操动作、桨板上的后空翻（涉及浮力和刚性）以及花样滑冰的三周跳（甚至头上还顶着一只猫）。这种对物理规律的内在理解，使得Sora 2生成的视频在动作连贯性、物体交互的真实感以及世界状态的持久性方面，都远超其前身 。

除了物理模拟的增强，Sora 2的另一项革命性进步是集成了**原生音频生成能力**。以往的AI视频模型大多生成无声视频，或者需要后期通过其他工具配音。而Sora 2能够同步生成与画面完美匹配的对话、环境音效和背景音乐 。例如，在一个生成的视频中，角色的嘴唇动作能与生成的语音精确同步，脚步声、物体碰撞声等音效也能与画面中的动作一一对应 。这种音视频同步生成能力，使得Sora 2从一个“无声电影”时代的AI，跃升为能够进行“多模态叙事”的强大工具，极大地增强了生成内容的沉浸感和表现力 。这一特性对于内容创作者而言意义重大，因为它将视频制作的多个环节（视觉生成、音效设计、配音）整合到了一个统一的、由文本驱动的流程中，显著降低了高质量视频内容的创作门槛 。

Sora 2的发布不仅是一个技术模型的更新，更伴随着一个全新的产品生态。OpenAI推出了名为“Sora”的独立iOS应用，该应用内置了一个类似TikTok的垂直短视频信息流，所有内容均由Sora 2模型生成 。这一战略转变显示了OpenAI意图将AI视频生成技术直接推向消费市场，并通过社交互动来激发创意和数据飞轮 。该应用还引入了一个名为 **“Cameo”（客串）** 的创新功能，允许用户上传一段自己的短视频，经过身份验证后，就可以将自己的形象和声音“植入”到任何AI生成的场景中 。这一功能不仅展示了Sora 2在保持角色一致性方面的强大能力，也为个性化内容创作和互动娱乐开辟了新的可能性。然而，这种强大的生成能力也引发了关于深度伪造（deepfake）和内容真实性的担忧，OpenAI为此实施了一系列安全措施，包括内容审核、可见水印以及严格的身份使用政策，以防止技术被滥用 。

| 特性维度 | Sora 1 (2024-2025) | Sora 2 (2025年10月) | 升级幅度与分析 |
| :--- | :--- | :--- | :--- |
| **核心能力** | 文本到视频；图像到视频；视频到视频 | 文本/图像到视频，集成同步音频（对话/音效） | **革命性升级**：从纯视觉生成到多模态音视频同步叙事，实现了完整的视听体验。 |
| **物理真实感** | 基础运动，存在物体变形、瞬移等问题 | 显著增强，遵循重力、动量、浮力等物理定律 | **质的飞跃**：模型从“看起来对”转向“物理上对”，能模拟真实世界的成功与失败，如篮球反弹 。 |
| **可控性与一致性** | 镜头控制有限，多镜头/角色一致性差 | 更好的指令遵循、多镜头连续性和角色一致性 | **大幅提升**：能够处理复杂的、多场景的提示，并保持世界状态和角色外观的稳定，为叙事提供了基础 。 |
| **音频生成** | 无原生音频，生成无声视频 | 原生、同步的音频生成（对话、环境音、音效） | **从无到有**：这是Sora 2最显著的新功能之一，使AI视频从“默片”时代进入“有声电影”时代 。 |
| **创新功能** | 无 | 引入“Cameo”功能，允许用户将自己或他人（经同意）的形象和声音植入视频 | **开创性功能**：极大地增强了个性化创作和社交互动性，展示了模型在身份保持方面的能力 。 |
| **产品形态** | 主要通过ChatGPT订阅提供网页访问 | 推出独立iOS应用“Sora”，内置社交信息流；提供网页访问；计划推出API | **生态扩展**：从单一工具向平台化、社交化产品演进，旨在构建创作者社区和数据飞轮 。 |
| **生成时长/分辨率** | 用户端产品约20秒，1080p | 用户端产品约20秒，1080p（Pro用户） | **基本持平**：尽管在内部研究中Sora 2能生成更长视频，但面向公众的产品在发布初期保持了与前代相似的时长和分辨率限制，重点在于提升质量而非单纯增加长度 。 |
| **安全与溯源** | 基础的内容审核和C2PA溯源 | 扩展的多模态审核，更严格的未成年人/形象保护，增强的溯源信号 | **持续加强**：针对更强大的生成能力和Cameo功能，OpenAI升级了安全栈，以应对深度伪造等潜在风险 。 |

#### 1.1.3 Meta V-JEPA 2：面向机器人规划的自监督视频世界模型

2025年6月11日，Meta AI发布了其最新的世界模型——**V-JEPA 2（Video Joint Embedding Predictive Architecture 2）** ，该模型在物理世界的视觉理解和预测方面取得了业界领先的性能，并能够实现对新环境和新物体的零样本机器人规划和控制 。V-JEPA 2的核心技术基于Meta首席AI科学家Yann LeCun所倡导的联合嵌入预测架构（JEPA）。与传统的生成模型（如扩散模型）试图预测每一个像素不同，JEPA模型通过在低维的“潜在空间”（latent space）中预测被遮蔽部分的抽象、高层级表征（embeddings）来进行学习。这种方法迫使模型专注于学习视频中物体的物理属性、动态变化和时空因果关系，而忽略了不可预测的、低层次的像素细节，从而提高了学习效率和泛化能力 。

V-JEPA 2的训练过程采用了创新的两阶段范式。第一阶段是 **“无动作预训练”（Action-Free Pre-training）** ，模型在超过一百万小时的未标注公开网络视频（VideoMix22M数据集）上进行学习，以建立对物理世界动态（如物体恒存性、重力、运动轨迹）的广泛、被动理解。在这一阶段，模型的视频编码器权重被冻结，以锁定其学到的通用知识 。第二阶段是 **“动作条件后训练”（Action-Conditioned Post-training）** ，一个专门的、名为V-JEPA 2-AC（Action-Conditioned）的版本，在少量（少于62小时）的机器人控制数据（如Droid数据集）上进行微调。在这一阶段，模型学习将抽象的世界知识与智能体动作的因果效应联系起来，即理解“如果我这样做，会发生什么”的逻辑。这种高效的两阶段训练方法，使得V-JEPA 2能够在仅有极少动作数据的情况下，实现强大的零样本机器人控制能力 。

V-JEPA 2的发布，标志着Meta在“世界模型 + 多模态 + 具身智能”战略方向上迈出了关键一步。该模型不仅是一个强大的视频理解工具，更是一个能够赋能机器人等物理智能体的“大脑”。通过将V-JEPA 2与多模态大型语言模型（MLLM）结合，可以构建出能够同时理解语言、视觉和物理规律的复杂AI系统 。为了推动整个领域的研究，Meta还开源了V-JEPA 2的相关代码和三个全新的基准测试，用于评估现有模型从视频中推理物理世界的能力 。这些举措旨在吸引更多研究者加入世界模型的探索，加速技术的迭代和应用。V-JEPA 2的成功，证明了通过自监督学习从海量视频中学习物理规律的可行性，为实现更高级别的机器智能和真正有用的具身AI代理铺平了道路。

### 1.2 AI智能体：在虚拟世界中通过强化学习掌握复杂技能

随着世界模型技术的成熟，AI智能体（AI Agents）的研究也进入了一个新的阶段。这些智能体不再局限于在简单的、规则明确的环境中执行任务，而是开始在由世界模型生成的、高度逼真且动态变化的虚拟世界中进行学习和训练。这种结合带来了巨大的优势：首先，世界模型提供了无限多样且可精确控制的环境，使得智能体能够接触到在现实世界中难以获取的、覆盖“长尾”分布的复杂场景，从而极大地提升了其泛化能力和鲁棒性。其次，在虚拟世界中进行训练成本低廉、安全性高，允许智能体通过大规模的试错学习来掌握复杂技能，而无需担心物理损坏或安全风险。2025年，以Meta的V-JEPA 2和NVIDIA的Cosmos-Reason模型为代表的研究成果，展示了AI智能体在视觉理解、物理推理和机器人规划方面取得的显著进展。这些智能体能够利用世界模型来预测行动的后果、规划多步任务，甚至在与现实世界高度相似的新环境中实现零样本（zero-shot）操作，这标志着我们正朝着构建能够真正在物理世界中自主行动和解决问题的通用AI智能体迈出了坚实的一步。

#### 1.2.1 DeepMind Dreamer 4：在可扩展世界模型中训练的智能体

在2025年9月，Google DeepMind发布了一项突破性研究，介绍了名为**Dreamer 4**的新型AI智能体，该智能体能够在完全离线的情况下，仅通过观看预录制的视频，在复杂的《我的世界》（Minecraft）游戏中学会并完成获取钻石这一极具挑战性的任务 。这一成就标志着世界模型（World Models）和强化学习（Reinforcement Learning, RL）领域的一个重大飞跃，因为它首次证明了智能体可以在不与真实环境进行任何交互的情况下，通过纯粹的“想象训练”（Imagination Training）来掌握复杂的、长周期的任务 。Dreamer 4的核心创新在于其构建了一个快速、准确且可扩展的内部世界模型，智能体在这个模型中进行强化学习，从而避免了在现实世界或复杂模拟环境中进行昂贵且耗时的试错过程 。这种方法对于机器人技术、自动驾驶等需要安全高效训练的应用领域具有极其重要的意义 。

Dreamer 4的成功关键在于其全新的架构和训练范式。与之前依赖循环状态空间模型（RSSM）的Dreamer版本（如DreamerV3）不同，Dreamer 4采用了高效的**Transformer架构**来处理高分辨率的视频数据，从而能够捕捉更长期的依赖关系 。其世界模型由两个主要部分组成：一个“因果分词器”（Causal Tokenizer），用于将高维视频帧压缩成连续的潜在表示；以及一个“交互式动力学模型”（Interactive Dynamics Model），用于在给定当前状态和动作的情况下预测未来的世界状态 。为了实现高效的实时交互，研究人员提出了一种名为 **“捷径强制”（Shortcut Forcing）** 的新型训练目标，该目标基于流匹配（Flow Matching）和捷径模型（Shortcut Models）的思想，使得模型能够在极少的采样步数内生成高质量的视频帧，从而在单个GPU上实现超过**20帧每秒**的实时生成速度 。这种高效的生成能力对于在“想象”中训练智能体至关重要，因为它允许智能体在内部世界模型中进行数百万次的模拟 rollout，从而有效地学习和优化其策略 。

Dreamer 4的训练过程分为三个阶段，完全基于离线数据 。第一阶段是“世界模型预训练”，在大规模的视频数据集（包括大量无标签视频）上预训练分词器和动力学模型，学习世界的通用视觉和物理规律。第二阶段是“智能体微调”，在带有动作和任务标签的数据上，向世界模型中引入策略和奖励头，并通过行为克隆（Behavioral Cloning）进行微调。第三阶段是“想象力训练”，冻结世界模型，完全在其生成的“梦境”中，使用强化学习（如PMPO）来优化策略头，以最大化学习到的奖励 。整个过程不与真实环境发生任何交互。在Minecraft钻石挑战中，Dreamer 4仅使用了OpenAI的VPT（Video PreTraining）智能体所用数据的**1%**，就成功地在**0.7%**的评估运行中获得了钻石，而VPT甚至未能可靠地进入铁器时代 。这一结果不仅展示了Dreamer 4卓越的数据效率，也证明了其世界模型能够学习到足够准确和通用的物理规律，以支持复杂任务的规划和执行 。

#### 1.2.2 VAGEN：通过强化学习提升多轮视觉语言智能体的世界模型推理能力

2025年10月，由微软研究院、西北大学、斯坦福大学等多机构联合研究团队发布了一项名为**VAGEN（Reinforcing World Model Reasoning for Multi-Turn VLM Agents）** 的创新研究，该研究旨在通过强化学习（RL）提升视觉-语言模型（VLM）智能体在复杂环境中的世界模型推理能力 。与主要处理文本状态的纯语言模型（LLM）智能体不同，VLM智能体需要应对复杂的视觉观察，这引入了部分可观测性（partial observability）的挑战，并对智能体的世界建模能力提出了更高的要求。VAGEN的核心研究问题是：VLM智能体能否通过显式的视觉状态推理来构建其内部世界模型？为了回答这个问题，研究团队将智能体的推理过程架构化，并通过强化学习对其进行奖励，将其形式化为一个部分可观测马尔可夫决策过程（POMDP） 。

VAGEN研究发现，将智能体的推理过程分解为两个关键部分—— **“状态估计”（State Estimation，即“当前状态是什么？”）和“转换建模”（Transition Modeling，即“接下来会发生什么？”）** ——对于成功至关重要。通过对五种不同的推理策略进行比较，研究证明了这种分解的有效性。此外，研究还深入探讨了智能体如何表示其内部信念，发现最优的表示方式取决于具体任务：自然语言在捕捉通用任务中的语义关系方面表现出色，而结构化格式则在精确操作和控制任务中不可或缺 。基于这些洞察，研究团队设计了一种 **“世界建模奖励”（World Modeling Reward）** ，为准确的状态预测提供密集的、逐轮的监督信号，并引入了 **“双层广义优势估计”（Bi-Level General Advantage Estimation, Bi-Level GAE）** 方法，以实现与轮次相关的信用分配。通过这种方式，一个仅有**30亿参数**的VLM智能体在五个不同的智能体基准测试中取得了**0.82**的平均分，相较于其未训练的版本（0.21）提升了3倍，并超过了GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）等专有推理模型 。

VAGEN框架本身也是一个可扩展的系统，用于在多样化的视觉环境中训练和分析多轮VLM智能体。该框架的代码和数据已公开发布，为后续研究提供了宝贵的资源 。这项研究的意义在于，它不仅提出了一种有效的方法来提升VLM智能体的推理能力，还揭示了世界模型在智能体决策中的核心作用。通过显式地奖励世界建模行为，VAGEN使得智能体能够更好地理解其环境，从而做出更明智的决策。这一成果对于推动具身智能、机器人技术以及更广泛的AI智能体应用具有重要意义。VAGEN的研究论文已被NeurIPS 2025接收，进一步证明了其在学术界的影响力 。

## 2. 基础理论：强化学习与深度学习理论的深化

### 2.1 强化学习在大型语言模型中的应用与理论分析

2025年，强化学习（Reinforcement Learning, RL）与大型语言模型（LLM）的结合已成为AI领域最热门的研究方向之一，其影响力贯穿了从模型训练到应用部署的整个生命周期 。这种融合不仅推动了技术边界的拓展，也为解决LLM的核心挑战，如对齐人类价值观、提升推理能力和增强可控性，提供了全新的理论框架和实用工具。特别是在2025年10月的中国计算机大会（CNCC2025）上，强化学习及其在大模型中的应用被列为核心Tutorial主题之一，凸显了其在学术界和工业界的重要地位 。

#### 2.1.1 强化学习重塑LLM推理能力的理论分析

随着大型语言模型（LLMs）能力的不断增强，如何通过强化学习（RL）进一步提升其推理能力，已成为理论机器学习领域的前沿课题。一篇于2025年6月发布的论文《Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection》对此进行了深入的理论分析。该研究的核心在于探讨RL训练动态如何通过 **“模式选择”（Pattern Selection）** 机制来重塑LLM的推理能力。模式选择指的是模型在学习过程中，从众多可能的解决方案或推理路径中，选择并强化那些更有效、更鲁棒的模式。这一理论框架试图解释，
